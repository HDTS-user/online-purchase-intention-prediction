{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b373cd",
   "metadata": {},
   "source": [
    "# AWS Marketplace Product Usage Demonstration - Online-purchasing-intention-prediction Model Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f62c9c",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9d62fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 prefixes\n",
    "common_prefix = \"<your S3 bucket name>\"\n",
    "batch_inference_input_prefix = \"purchasing_intention_prediction/input\"\n",
    "batch_inference_output_prefix = \"purchasing_intention_prediction/output\"\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5990a",
   "metadata": {},
   "source": [
    "## 2. Create the session\n",
    "\n",
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd723e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff902a",
   "metadata": {},
   "source": [
    "## 3. Create Model\n",
    "\n",
    "Now we use the above Model Package to create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7e3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = '<your sagemaker model package arn>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148f1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'online-purchasing-intention-prediction'\n",
    "\n",
    "content_type = 'application/json'\n",
    "\n",
    "real_time_inference_instance_type = 'ml.m5.large'\n",
    "batch_transform_inference_instance_type = 'ml.m5.large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccca6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wrapper(endpoint, session):\n",
    "    return sage.predictor.Predictor(endpoint, session, content_type)\n",
    "\n",
    "# Create a deployable model from the model package\n",
    "model = ModelPackage(role=role,\n",
    "                    model_package_arn=model_package_arn,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    predictor_cls = predict_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036dd53e",
   "metadata": {},
   "source": [
    "## 4. Create-an-endpoint-and-perform-real-time-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54776675",
   "metadata": {},
   "source": [
    "#### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85108241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "#Deploy the model\n",
    "predictor = model.deploy(1,real_time_inference_instance_type,endpoint_name = model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19057954",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0e3b5",
   "metadata": {},
   "source": [
    "#### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'input/test.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3388352",
   "metadata": {},
   "source": [
    "#### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b7df776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"ContentType\": \"text/csv; charset=utf-8\",\r\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $model_name \\\n",
    "    --body fileb://$file_name \\\n",
    "    --content-type $content_type \\\n",
    "    --region $sagemaker_session.boto_region_name \\\n",
    "    output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1576a24",
   "metadata": {},
   "source": [
    "#### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b571ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'r') as f:\n",
    "    output = json.load(f)\n",
    "print(json.dumps(output,indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2759bb",
   "metadata": {},
   "source": [
    "#### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e65b1",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37a53654",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32493b91",
   "metadata": {},
   "source": [
    "## 5. Perform batch inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a0404",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770f2dd",
   "metadata": {},
   "source": [
    "### A. Configure the input S3 bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "198d4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_input_folder = 'input'\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder,common_prefix,batch_inference_input_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf5682",
   "metadata": {},
   "source": [
    "### B. Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e99e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: online-purchasing-intention-prediction-2024-06-05-07-29-24-908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n",
      "\u001b[34mStarting the inference server with 2 workers.\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [9] [INFO] Starting gunicorn 22.0.0\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [9] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:52 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[35mStarting the inference server with 2 workers.\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [9] [INFO] Starting gunicorn 22.0.0\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [9] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:52 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mInvoked with 1 records\u001b[0m\n",
      "\u001b[35mInvoked with 1 records\u001b[0m\n",
      "\u001b[32m2024-06-05T07:33:58.898:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Jun/2024:07:33:59 +0000] \"POST /invocations HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [05/Jun/2024:07:33:59 +0000] \"POST /invocations HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mStarting the inference server with 2 workers.\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [9] [INFO] Starting gunicorn 22.0.0\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [9] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:51 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[34m[2024-06-05 07:33:52 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[35mStarting the inference server with 2 workers.\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [9] [INFO] Starting gunicorn 22.0.0\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [9] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:51 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[35m[2024-06-05 07:33:52 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [05/Jun/2024:07:33:58 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mInvoked with 1 records\u001b[0m\n",
      "\u001b[35mInvoked with 1 records\u001b[0m\n",
      "\u001b[32m2024-06-05T07:33:58.898:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Jun/2024:07:33:59 +0000] \"POST /invocations HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [05/Jun/2024:07:33:59 +0000] \"POST /invocations HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transformer = model.transformer(1,batch_transform_inference_instance_type,output_path=\"s3://\"+common_prefix+\"/\"+batch_inference_output_prefix+\"/\")\n",
    "transformer.transform(transform_input,content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c355c0",
   "metadata": {},
   "source": [
    "### C. Download the file from output S3 bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a11848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "s3_conn = boto3.client(\"s3\")\n",
    "with open('output2.txt','wb') as f:\n",
    "    s3_conn.download_fileobj(common_prefix,batch_inference_output_prefix+'/'+'test.json.out',f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee0b87",
   "metadata": {},
   "source": [
    "### D. Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51458573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('output2.txt','r') as f:\n",
    "    output = json.load(f)\n",
    "print(json.dumps(output,indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4932a7",
   "metadata": {},
   "source": [
    "### 6. Clean-up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e405e",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba326d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: online-purchasing-intention-prediction-2024-06-05-07-29-24-106\n"
     ]
    }
   ],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2e780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
